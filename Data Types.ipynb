{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data representations for neural networks\n",
    "Matrices, which are 2D tensors: tensors are a generalization of matrices to an arbitrary number of dimensions\n",
    "(note that in the context of tensors, a dimension is often called an axis).\n",
    "\n",
    "\n",
    "\n",
    "## Scalar (0D tensors)\n",
    "A tensor that contains only one number is called a scalar (or scalar tensor, or 0-dimensional\n",
    "tensor, or 0D tensor). In Numpy, a float32 or float64 number is a scalar tensor (or scalar\n",
    "array). You can display the number of axes of a Numpy tensor via the ndim attribute; a scalar\n",
    "tensor has 0 axes (ndim == 0). The number of axes of a tensor is also called its rank.\n",
    "Here’s a Numpy scalar:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "12\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "x = np.array(12)\n",
    "print(x)\n",
    "x.ndim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vector(1D tensors)\n",
    "An array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly\n",
    "one axis. Following is a Numpy vector:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[12  3  6 14]\n1\n(4,)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = np.array([12, 3, 6, 14])\n",
    "print(x)\n",
    "print(x.ndim)\n",
    "print(x.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This vector has five entries and so is called a 5-dimensional vector. \n",
    "\n",
    "Don’t confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its\n",
    "axis, whereas a 5D tensor has five axes (and may have any number of dimensions\n",
    "along each axis). \n",
    "\n",
    "Dimensionality can denote either the number of entries along a specific\n",
    "axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a\n",
    "5D tensor), which can be confusing at times. In the latter case, it’s technically more\n",
    "correct to talk about a tensor of rank 5 (the rank of a tensor being the number of axes),\n",
    "but the ambiguous notation 5D tensor is common regardless.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Matrices (2D tensors)\n",
    "\n",
    "An array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to\n",
    "rows and columns). You can visually interpret a matrix as a rectangular grid of numbers.\n",
    "This is a Numpy matrix:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "2\n(3, 5)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "[6, 79, 3, 35, 1],\n",
    "[7, 80, 4, 36, 2]])\n",
    "\n",
    "print(x.ndim)\n",
    "\n",
    "print(x.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The entries from the first axis are called the rows, and the entries from the second axis\n",
    "are called the columns. In the previous example, [5, 78, 2, 34, 0] is the first row of x,\n",
    "and [5, 6, 7] is the first column.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 3D tensors and higher-dimensional tensors\n",
    "\n",
    "If you pack such matrices in a new array, you obtain a 3D tensor, which you can visually\n",
    "interpret as a cube of numbers. Following is a Numpy 3D tensor:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[[ 5 78  2 34  0]\n  [ 6 79  3 35  1]\n  [ 7 80  4 36  2]]\n\n [[ 5 78  2 34  0]\n  [ 6 79  3 35  1]\n  [ 7 80  4 36  2]]\n\n [[ 5 78  2 34  0]\n  [ 6 79  3 35  1]\n  [ 7 80  4 36  2]]]\n3\n(3, 3, 5)\nint32\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "                [6, 79, 3, 35, 1],\n",
    "                [7, 80, 4, 36, 2]],\n",
    "                [[5, 78, 2, 34, 0],\n",
    "                [6, 79, 3, 35, 1],\n",
    "                [7, 80, 4, 36, 2]],\n",
    "                [[5, 78, 2, 34, 0],\n",
    "                [6, 79, 3, 35, 1],\n",
    "                [7, 80, 4, 36, 2]]])\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(x.ndim)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "print(x.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By packing 3D tensors in an array, you can create a 4D tensor, and so on. In deep learning,\n",
    "you’ll generally manipulate tensors that are 0D to 4D, although you may go up to\n",
    "5D if you process video data.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Key attributes\n",
    "\n",
    "A tensor is defined by three key attributes:\n",
    "\n",
    "* Number of axes (rank)—For instance, a 3D tensor has three axes, and a matrix has\n",
    "two axes. This is also called the tensor’s ndim in Python libraries such as Numpy.\n",
    "\n",
    "* Shape—This is a tuple of integers that describes how many dimensions the tensor\n",
    "has along each axis. For instance, the previous matrix example has shape\n",
    "(3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape\n",
    "with a single element, such as (5,), whereas a scalar has an empty shape, ().\n",
    "\n",
    "* Data type (usually called dtype in Python libraries)—This is the type of the data\n",
    "contained in the tensor; for instance, a tensor’s type could be float32, uint8,\n",
    "float64, and so on. On rare occasions, you may see a char tensor. Note that\n",
    "string tensors don’t exist in Numpy (or in most other libraries), because tensors\n",
    "live in preallocated, contiguous memory segments: and strings, being variable\n",
    "length, would preclude the use of this implementation.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "3\n(60000, 28, 28)\nuint8\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_images.ndim)\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "print(train_images.dtype)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So what we have here is a 3D tensor of 8-bit integers. More precisely, it’s an array of 60,000 matrices of 28 × 8 integers.\n",
    "\n",
    "\n",
    "## Manipulating tensors in Numpy\n",
    "\n",
    "In the previous example, we selected a specific digit alongside the first axis using the\n",
    "syntax train_images[i]. Selecting specific elements in a tensor is called tensor slicing.\n",
    "Let’s look at the tensor-slicing operations you can do on Numpy arrays.\n",
    "The following example selects digits #10 to #100 (#100 isn’t included) and puts\n",
    "them in an array of shape (90, 28, 28)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(90, 28, 28)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "my_slice=train_images[10:100]\n",
    "print(my_slice.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It’s equivalent to this more detailed notation, which specifies a start index and stop\n",
    "index for the slice along each tensor axis. Note that : is equivalent to selecting the\n",
    "entire axis:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100, :, :]  #Equivalent to the previous example\n",
    "print(my_slice.shape)\n",
    "\n",
    "my_slice = train_images[10:100, 0:28, 0:28] #Also equivalent to the previous example\n",
    "print(my_slice.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In general, you may select between any two indices along each tensor axis. For\n",
    "instance, in order to select 14 × 14 pixels **in the bottom-right corner of all images**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "my_slice = train_images[:, 14:, 14:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It’s also possible to use negative indices. Much like negative indices in Python lists,\n",
    "they indicate a position relative to the end of the current axis. In order to crop the\n",
    "images to patches of 14 × 14 pixels centered in the middle, you do this:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_slice2 = train_images[:, 7:-7, 7:-7]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The notion of data batches\n",
    "\n",
    "In general, the first axis (axis 0, because indexing starts at 0) in all data tensors you’ll\n",
    "come across in deep learning will be the samples axis (sometimes called the samples\n",
    "dimension). \n",
    "\n",
    "In the MNIST example, samples are images of digits.\n",
    "In addition, deep-learning models don’t process an entire dataset at once; rather,\n",
    "they break the data into small batches. Concretely, here’s one batch of our MNIST digits,\n",
    "with batch size of 128:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(128, 28, 28)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 22
    }
   ],
   "source": [
    "batch = train_images[:128]\n",
    "batch.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here’s the next batch:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(128, 28, 28)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 23
    }
   ],
   "source": [
    "batch1 = train_images[128:256]\n",
    "batch1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the nth batch:\n",
    "\n",
    "```python\n",
    "batch2 = train_images[128 * n:128 * (n + 1)]\n",
    "```\n",
    "When considering such a batch tensor, the first axis (axis 0) is called the batch axis or\n",
    "batch dimension. This is a term you’ll frequently encounter when using Keras and other\n",
    "deep-learning libraries.\n",
    "\n",
    "## Real-world examples of data tensors\n",
    "Let’s make data tensors more concrete with a few examples similar to what you’ll\n",
    "encounter later. The data you’ll manipulate will almost always fall into one of the following\n",
    "categories:\n",
    "* Vector data—2D tensors of shape (samples, features)\n",
    "* **Timeseries data or sequence data—3D tensors of shape (samples, timesteps,\n",
    "features)**\n",
    "* Images—4D tensors of shape (samples, height, width, channels) or (samples,\n",
    "channels, height, width). (samples, height, width, color_depth)> Tensorflow\n",
    "\n",
    "* Video—5D tensors of shape (samples, frames, height, width, channels) or\n",
    "(samples, frames, channels, height, width)\n",
    "\n",
    "## Vector data\n",
    "This is the most common case. In such a dataset, each single data point can be encoded\n",
    "as a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of\n",
    "vectors), where the first axis is the samples axis and the second axis is the features axis.\n",
    "Let’s take a look at two examples:\n",
    "* An actuarial dataset of people, where we consider each person’s age, ZIP code,\n",
    "and income. Each person can be characterized as a vector of 3 values, and thus\n",
    "an entire dataset of 100,000 people can be stored in a 2D tensor of shape\n",
    "(100000, 3). *(number of samples, number of features)*\n",
    "\n",
    "\n",
    "* A dataset of text documents, where we represent each document by the counts\n",
    "of how many times each word appears in it (out of a dictionary of 20,000 common\n",
    "words). Each document can be encoded as a vector of 20,000 values (one\n",
    "count per word in the dictionary), and thus an entire dataset of 500 documents\n",
    "can be stored in a tensor of shape (500, 20000). *(Words== Features)*\n",
    "\n",
    "## Timeseries data or sequence data\n",
    "Whenever time matters in your data (or the notion of sequence order), it makes sense\n",
    "to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a\n",
    "sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D\n",
    "tensor \n",
    "\n",
    "**The time axis is always the second axis (axis of index 1)**, by convention. Let’s look at a\n",
    "few examples:\n",
    "\n",
    "* A dataset of stock prices. Every minute, we store the current price of the stock,\n",
    "the highest price in the past minute, and the lowest price in the past minute.\n",
    "Thus every minute is encoded as a 3D vector, an entire day of trading is\n",
    "encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading\n",
    "day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\n",
    "390, 3). Here, each sample would be one day’s worth of data. *(samples, timesteps,\n",
    "features)*\n",
    "\n",
    "\n",
    "* A dataset of tweets, where we encode each tweet as a sequence of 280 characters\n",
    "out of an alphabet of 128 unique characters. In this setting, each character can\n",
    "be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry\n",
    "at the index corresponding to the character). Then each tweet can be encoded\n",
    "as a 2D tensor of shape (280, 128), and a dataset of 1 million tweets can be\n",
    "stored in a tensor of shape (1000000, 280, 128).\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "In our initial example, we were building our network by stacking Dense layers on\n",
    "top of each other. A Keras layer instance looks like this:\n",
    "```python\n",
    "keras.layers.Dense(512, activation='relu')\n",
    "```\n",
    "\n",
    "This layer can be interpreted as a function, which takes as input a 2D tensor and\n",
    "returns another 2D tensor—a new representation for the input tensor. Specifically, the\n",
    "function is as follows (where W is a 2D tensor and b is a vector, both attributes of the\n",
    "layer):\n",
    "\n",
    "```python\n",
    "output = relu(dot(W, input) + b)\n",
    "```\n",
    "Let’s unpack this. We have three tensor operations here: a dot product (dot) between\n",
    "the input tensor and a tensor named W; an addition (+) between the resulting 2D tensor\n",
    "and a vector b; and, finally, a relu operation. relu(x) is max(x, 0).\n",
    "\n",
    "## Element-wise operations\n",
    "\n",
    "\n",
    "The relu operation and addition are element-wise operations: operations that are\n",
    "applied independently to each entry in the tensors being considered. This means\n",
    "these operations are highly amenable to massively parallel implementations (vectorized\n",
    "implementations, a term that comes from the vector processor supercomputer architecture\n",
    "from the 1970–1990 period). If you want to write a naive Python implementation\n",
    "of an element-wise operation, you use a for loop, as in this naive\n",
    "implementation of an element-wise relu operation:\n",
    "\n",
    "\n",
    "```python\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2   #<----- x is a 2D Numpy tensor     (assert=ileri surmek)\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x\n",
    "```\n",
    "\n",
    "You do the same for addition:\n",
    "\n",
    "```python\n",
    "def naive_add(x, y):    \n",
    "    assert len(x.shape) == 2     #<---------- x and y are 2D Numpy tensors.\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()                #<---------- Avoid overwriting the input tensor.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x\n",
    "```\n",
    "On the same principle, you can do element-wise multiplication, subtraction, and so on.\n",
    "In practice, when dealing with Numpy arrays, these operations are available as welloptimized\n",
    "built-in Numpy functions, which themselves delegate the heavy lifting to a\n",
    "Basic Linear Algebra Subprograms (BLAS) implementation if you have one installed\n",
    "(which you should). BLAS are low-level, highly parallel, efficient tensor-manipulation\n",
    "routines that are typically implemented in Fortran or C.\n",
    "So, in Numpy, you can do the following element-wise operation, and it will be blazing\n",
    "fast:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "z= x + y  #Element-wise addition\n",
    "z=np.maximum(z,0,)  #Element-wise relu\n",
    "```\n",
    "\n",
    "## Broadcasting\n",
    "\n",
    "What happens with addition when the shapes of the two tensors\n",
    "being added differ?\n",
    "When possible, and if there’s no ambiguity, the smaller tensor will be broadcasted to\n",
    "match the shape of the larger tensor. Broadcasting consists of two steps:\n",
    "\n",
    "1.  Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\n",
    "the larger tensor.\n",
    "\n",
    "2.  The smaller tensor is repeated alongside these new axes to match the full shape\n",
    "of the larger tensor.\n",
    "\n",
    "Let’s look at a concrete example. Consider X with shape (32, 10) and y with shape\n",
    "(10,). \n",
    "* First, we add an empty first axis to y, whose shape becomes (1, 10). \n",
    "* Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape\n",
    "(32, 10), where Y[i, :] == y for i in range(0, 32). At this point, we can proceed to\n",
    "add X and Y, because they have the same shape.\n",
    "\n",
    "The repetition operation is entirely virtual: it happens at the algorithmic\n",
    "level rather than at the memory level.\n",
    "\n",
    "With broadcasting, you can generally apply two-tensor element-wise operations if one\n",
    "tensor has shape (a, b, … n, n + 1, … m) and the other has shape (n, n + 1, … m). The\n",
    "broadcasting will then automatically happen for axes a through n - 1.\n",
    "The following example applies the element-wise maximum operation to two tensors\n",
    "of different shapes via broadcasting:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.random((64, 3, 32, 10))  #x is a random tensor with shape (64, 3, 32, 10).\n",
    "y = np.random.random((32, 10)) #y is a random tensor with shape (32, 10).\n",
    "z = np.maximum(x, y) #The output z has shape (64, 3, 32, 10) like x.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Tensor dot\n",
    "\n",
    "Two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot product.\n",
    "\n",
    "\n",
    "You can also take the dot product between a matrix x and a vector y, which returns a vector where the coefficients are the dot products between y and the rows of x.\n",
    "\n",
    "\n",
    "Note that as soon as one of the two tensors has an ndim greater than 1, dot is no longer\n",
    "symmetric, which is to say that dot(x, y) isn’t the same as dot(y, x).\n",
    "Of course, a dot product generalizes to tensors with an arbitrary number of axes.\n",
    "The most common applications may be the dot product between two matrices. You\n",
    "can take the dot product of two matrices x and y (dot(x, y)) if and only if\n",
    "x.shape[1] == y.shape[0]. The result is a matrix with shape (x.shape[0],\n",
    "y.shape[1]), where the coefficients are the vector products between the rows of x\n",
    "and the columns of y.\n",
    "\n",
    "\n",
    "\n",
    "## Tensor reshaping\n",
    "\n",
    "A third type of tensor operation that’s essential to understand is tensor reshaping.\n",
    "Although it wasn’t used in the Dense layers in our first neural network example, we\n",
    "used it when we preprocessed the digits data before feeding it into our network:\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "Reshaping a tensor means rearranging its rows and columns to match a target shape.\n",
    "Naturally, the reshaped tensor has the same total number of coefficients as the initial\n",
    "tensor. Reshaping is best understood via simple examples:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = np.array([[0., 1.],\n",
    "[2., 3.],\n",
    "[4., 5.]])\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x1=x.reshape((6,1))\n",
    "print(x1)\n",
    "\n",
    "x2=x.reshape((2,3))\n",
    "print(x2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(3, 2)\n[[0.]\n [1.]\n [2.]\n [3.]\n [4.]\n [5.]]\n[[0. 1. 2.]\n [3. 4. 5.]]\n"
     ],
     "output_type": "stream"
    }
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}